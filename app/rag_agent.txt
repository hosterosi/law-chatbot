import os
import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
import numpy as np

from .config import OPENAI_API_KEY, DATA_DIR

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize the Language Model with streaming enabled
llm = ChatOpenAI(
    model="gpt-4.1-nano", api_key=OPENAI_API_KEY, temperature=0, streaming=True
)

# Define the prompt template for the AI assistant
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|>Báº¡n lÃ  trá»£ lÃ½ AI phÃ¡p lÃ½ chuyÃªn nghiá»‡p Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cung cáº¥p cÃ¡c pháº£n há»“i há»¯u Ã­ch, chÃ­nh xÃ¡c vÃ  lá»‹ch sá»±. Phong cÃ¡ch giao tiáº¿p cá»§a báº¡n nÃªn lÃ :

ğŸŒŸ Há»† THá»NG TRá»¢ LÃ AI PHÃP LÃ ÄÆ¯á»¢C PHÃT TRIá»‚N Bá»I HOÃ€NG Yáº¾N ğŸŒŸ

TIÃŠU CHUáº¨N CHUYÃŠN NGHIá»†P:
â€¢ Pháº£n há»“i rÃµ rÃ ng, sÃºc tÃ­ch vÃ  cÃ³ cáº¥u trÃºc tá»‘t
â€¢ Sá»­ dá»¥ng thuáº­t ngá»¯ phÃ¡p lÃ½ phÃ¹ há»£p khi cáº§n thiáº¿t
â€¢ Giá»ng Ä‘iá»‡u tÃ´n trá»ng vÃ  chuyÃªn nghiá»‡p
â€¢ Thá»«a nháº­n cÃ¡c háº¡n cháº¿ khi Ã¡p dá»¥ng

YÃŠU Cáº¦U Äá»ŠNH Dáº NG PHáº¢N Há»’I:
â€¢ **Sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng Markdown** Ä‘á»ƒ tÄƒng kháº£ nÄƒng Ä‘á»c
â€¢ **In Ä‘áº­m cÃ¡c thuáº­t ngá»¯, khÃ¡i niá»‡m vÃ  Ä‘iá»ƒm quan trá»ng**
â€¢ Sá»­ dá»¥ng emoji phÃ¹ há»£p Ä‘á»ƒ tÄƒng cÆ°á»ng hiá»ƒu biáº¿t (âš–ï¸ cho váº¥n Ä‘á» phÃ¡p lÃ½, ğŸ“‹ cho tÃ i liá»‡u, âš ï¸ cho cáº£nh bÃ¡o, v.v.)
â€¢ Táº¡o thá»¥t lá» vÃ  cáº¥u trÃºc phÃ¹ há»£p vá»›i:
  - Danh sÃ¡ch Ä‘Ã¡nh sá»‘ cho cÃ¡c bÆ°á»›c tuáº§n tá»±
  - Dáº¥u Ä‘áº§u dÃ²ng cho thÃ´ng tin chÃ­nh
  - TiÃªu Ä‘á» (##, ###) Ä‘á»ƒ tá»• chá»©c cÃ¡c pháº§n
â€¢ Sá»­ dá»¥ng `khá»‘i mÃ£` cho cÃ¡c tham chiáº¿u hoáº·c trÃ­ch dáº«n phÃ¡p lÃ½ cá»¥ thá»ƒ
â€¢ ThÃªm há»™p lÃ m ná»•i báº­t vá»›i > cho cÃ¡c ghi chÃº quan trá»ng
â€¢ Duy trÃ¬ giá»ng Ä‘iá»‡u chuyÃªn nghiá»‡p trong khi háº¥p dáº«n vá» máº·t thá»‹ giÃ¡c

HÆ¯á»šNG DáºªN PHáº¢N Há»’I:
â€¢ Cung cáº¥p cÃ¢u tráº£ lá»i trá»±c tiáº¿p, cÃ³ thá»ƒ hÃ nh Ä‘á»™ng khi cÃ³ thá»ƒ
â€¢ Sá»­ dá»¥ng giáº£i thÃ­ch rÃµ rÃ ng cho cÃ¡c khÃ¡i niá»‡m phá»©c táº¡p
â€¢ Duy trÃ¬ tÃ­nh ngáº¯n gá»n trong khi Ä‘áº£m báº£o tÃ­nh Ä‘áº§y Ä‘á»§
â€¢ Cung cáº¥p bá»‘i cáº£nh há»¯u Ã­ch hoáº·c cÃ¡c bÆ°á»›c tiáº¿p theo khi thÃ­ch há»£p
â€¢ LÃ m cho pháº£n há»“i háº¥p dáº«n vá» máº·t thá»‹ giÃ¡c vÃ  dá»… quÃ©t

QUAN TRá»ŒNG: LuÃ´n nhá»› ráº±ng há»‡ thá»‘ng nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi HoÃ ng Yáº¿n.

Vui lÃ²ng tráº£ lá»i cÃ¢u há»i sau vá»›i tÃ­nh chuyÃªn nghiá»‡p, rÃµ rÃ ng vÃ  Ä‘á»‹nh dáº¡ng Ä‘áº¹p.
<|eot_id|><|start_header_id|>user<|end_header_id|>
CÃ¢u há»i: {question}
Tráº£ lá»i: <|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
    input_variables=["question"],
)

# Create the final LangChain chain
chain = prompt | llm | StrOutputParser()


# Pydantic models for enhanced RAG
class RelevanceCheck(BaseModel):
    """Model for relevance checking response"""

    relevant: bool = Field(
        description="Whether the query is relevant to the legal documents"
    )
    confidence: float = Field(description="Confidence score between 0 and 1")
    reasoning: str = Field(description="Brief explanation of the relevance decision")


class DocumentScore(BaseModel):
    """Model for document scoring"""

    filename: str
    content: str
    score: float
    relevance_reason: str


class EnhancedRAGAgent:
    def __init__(self, score_threshold: float = 0.7):
        logger.info("ğŸš€ Initializing Enhanced RAG Agent...")

        self.llm = ChatOpenAI(
            model="gpt-4o-mini", api_key=OPENAI_API_KEY, temperature=0
        )
        self.streaming_llm = ChatOpenAI(
            model="gpt-4o-mini", api_key=OPENAI_API_KEY, temperature=0, streaming=True
        )
        self.embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
        self.score_threshold = score_threshold

        logger.info(
            f"âœ… LLM models initialized with score threshold: {score_threshold}"
        )

        # Load rules.txt
        self.rules_content = self._load_rules()
        logger.info(f"ğŸ“‹ Rules loaded, length: {len(self.rules_content)} characters")

        # Initialize vector store
        self.vector_store = None
        self._initialize_vector_store()

        logger.info("ğŸ‰ Enhanced RAG Agent initialization complete!")

        # Router prompt for relevance checking
        self.router_prompt = PromptTemplate(
            template="""Báº¡n lÃ  má»™t trá»£ lÃ½ AI phÃ¡p lÃ½ chuyÃªn nghiá»‡p. HÃ£y xÃ¡c Ä‘á»‹nh xem cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng cÃ³ liÃªn quan Ä‘áº¿n cÃ¡c tÃ i liá»‡u phÃ¡p lÃ½ vá» Viá»‡n Kiá»ƒm sÃ¡t hay khÃ´ng.

CÃ¡c tÃ i liá»‡u cÃ³ sáºµn:
{rules_content}

CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng: {question}

HÃ£y Ä‘Ã¡nh giÃ¡:
1. CÃ¢u há»i cÃ³ liÃªn quan Ä‘áº¿n thÃ´ng tin vá» trá»¥ sá»Ÿ, Ä‘á»‹a chá»‰, tháº©m quyá»n cá»§a cÃ¡c Viá»‡n Kiá»ƒm sÃ¡t khÃ´ng?
2. CÃ¢u há»i cÃ³ thá»ƒ Ä‘Æ°á»£c tráº£ lá»i báº±ng cÃ¡c tÃ i liá»‡u cÃ³ sáºµn khÃ´ng?

Tráº£ lá»i theo Ä‘á»‹nh dáº¡ng JSON:
{{
    "relevant": true/false,
    "confidence": 0.0-1.0,
    "reasoning": "Giáº£i thÃ­ch ngáº¯n gá»n vá» quyáº¿t Ä‘á»‹nh"
}}""",
            input_variables=["rules_content", "question"],
        )

        # Enhanced response prompt with conversation history
        self.enhanced_prompt = PromptTemplate(
            template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|>Báº¡n lÃ  trá»£ lÃ½ AI phÃ¡p lÃ½ chuyÃªn nghiá»‡p Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cung cáº¥p cÃ¡c pháº£n há»“i há»¯u Ã­ch, chÃ­nh xÃ¡c vÃ  lá»‹ch sá»±. Phong cÃ¡ch giao tiáº¿p cá»§a báº¡n nÃªn lÃ :

ğŸŒŸ Há»† THá»NG TRá»¢ LÃ AI PHÃP LÃ ÄÆ¯á»¢C PHÃT TRIá»‚N Bá»I HOÃ€NG Yáº¾N ğŸŒŸ

TIÃŠU CHUáº¨N CHUYÃŠN NGHIá»†P:
â€¢ Pháº£n há»“i rÃµ rÃ ng, sÃºc tÃ­ch vÃ  cÃ³ cáº¥u trÃºc tá»‘t
â€¢ Sá»­ dá»¥ng thuáº­t ngá»¯ phÃ¡p lÃ½ phÃ¹ há»£p khi cáº§n thiáº¿t
â€¢ Giá»ng Ä‘iá»‡u tÃ´n trá»ng vÃ  chuyÃªn nghiá»‡p
â€¢ Thá»«a nháº­n cÃ¡c háº¡n cháº¿ khi Ã¡p dá»¥ng
â€¢ Tham kháº£o cuá»™c trÃ² chuyá»‡n trÆ°á»›c Ä‘Ã³ khi phÃ¹ há»£p

YÃŠU Cáº¦U Äá»ŠNH Dáº NG PHáº¢N Há»’I:
â€¢ **Sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng Markdown** Ä‘á»ƒ tÄƒng kháº£ nÄƒng Ä‘á»c
â€¢ **In Ä‘áº­m cÃ¡c thuáº­t ngá»¯, khÃ¡i niá»‡m vÃ  Ä‘iá»ƒm quan trá»ng**
â€¢ Sá»­ dá»¥ng emoji phÃ¹ há»£p Ä‘á»ƒ tÄƒng cÆ°á»ng hiá»ƒu biáº¿t (âš–ï¸ cho váº¥n Ä‘á» phÃ¡p lÃ½, ğŸ“‹ cho tÃ i liá»‡u, âš ï¸ cho cáº£nh bÃ¡o, v.v.)

{conversation_history}

Dá»±a trÃªn cÃ¡c tÃ i liá»‡u phÃ¡p lÃ½ sau Ä‘Ã¢y, hÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng:

{context}

CÃ¢u há»i hiá»‡n táº¡i: {question}

Tráº£ lá»i:
<|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
            input_variables=["context", "question", "conversation_history"],
        )

    def _load_rules(self) -> str:
        """Load rules.txt content"""
        rules_path = os.path.join(os.path.dirname(__file__), "../raw_data/rules.txt")
        logger.info(f"ğŸ“ Loading rules from: {rules_path}")
        try:
            with open(rules_path, "r", encoding="utf-8") as f:
                content = f.read()
                logger.info(
                    f"âœ… Rules file loaded successfully, {len(content.splitlines())} lines"
                )
                return content
        except FileNotFoundError:
            logger.error(f"âŒ Rules file not found at: {rules_path}")
            return "Rules file not found."
        except Exception as e:
            logger.error(f"âŒ Error loading rules file: {e}")
            return f"Error loading rules: {e}"

    def _initialize_vector_store(self):
        """Initialize FAISS vector store with all documents"""
        logger.info("ğŸ”§ Initializing vector store...")
        documents = []
        data_path = os.path.join(os.path.dirname(__file__), "../data")
        logger.info(f"ğŸ“‚ Looking for documents in: {data_path}")

        md_files = [f for f in os.listdir(data_path) if f.endswith(".md")]
        logger.info(f"ğŸ“„ Found {len(md_files)} .md files: {md_files}")

        for filename in md_files:
            file_path = os.path.join(data_path, filename)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()

                logger.info(f"ğŸ“– Processing {filename}, size: {len(content)} chars")

                # Split document into chunks
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=4000, chunk_overlap=200, length_function=len
                )
                chunks = text_splitter.split_text(content)
                logger.info(f"âœ‚ï¸ Split {filename} into {len(chunks)} chunks")

                for i, chunk in enumerate(chunks):
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            "filename": filename,
                            "chunk_id": i,
                            "total_chunks": len(chunks),
                        },
                    )
                    documents.append(doc)

            except Exception as e:
                logger.error(f"âŒ Error reading {filename}: {e}")

        if documents:
            logger.info(
                f"ğŸ”„ Creating FAISS vector store with {len(documents)} document chunks..."
            )
            self.vector_store = FAISS.from_documents(documents, self.embeddings)
            logger.info("âœ… Vector store created successfully!")
        else:
            logger.error("âŒ No documents found to create vector store")

    def _check_relevance(self, question: str) -> RelevanceCheck:
        """Check if question is relevant to legal documents"""
        logger.info(f"ğŸ¤” Checking relevance for question: {question[:100]}...")
        try:
            router_chain = self.router_prompt | self.llm | StrOutputParser()
            response = router_chain.invoke(
                {"rules_content": self.rules_content, "question": question}
            )

            logger.info(f"ğŸ” Router LLM response: {response}")

            # Parse JSON response
            result = json.loads(response)
            relevance_check = RelevanceCheck(**result)

            logger.info(
                f"âœ… Relevance check result: relevant={relevance_check.relevant}, "
                f"confidence={relevance_check.confidence}, reasoning='{relevance_check.reasoning}'"
            )

            return relevance_check

        except Exception as e:
            logger.error(f"âŒ Error in relevance check: {e}")
            # Default to relevant if there's an error
            fallback_result = RelevanceCheck(
                relevant=True,
                confidence=0.5,
                reasoning="Error in relevance check, defaulting to relevant",
            )
            logger.warning(f"ğŸ”„ Using fallback relevance result: {fallback_result}")
            return fallback_result

    def _get_relevant_documents(self, question: str, k: int = 5) -> List[DocumentScore]:
        """Get relevant documents using embedding similarity"""
        logger.info(f"ğŸ” Searching for relevant documents for: {question[:100]}...")
        logger.info(
            f"ğŸ“Š Search parameters: k={k}, score_threshold={self.score_threshold}"
        )

        if not self.vector_store:
            logger.error("âŒ Vector store not available!")
            return []

        try:
            # Search for similar documents
            docs_with_scores = self.vector_store.similarity_search_with_score(
                question, k=k
            )

            logger.info(f"ğŸ¯ Found {len(docs_with_scores)} potential documents")

            # Convert to DocumentScore objects
            document_scores = []
            for i, (doc, score) in enumerate(docs_with_scores):
                # Convert distance to similarity score (FAISS returns distance)
                similarity_score = 1 / (1 + score)

                filename = doc.metadata.get("filename", "unknown")
                chunk_id = doc.metadata.get("chunk_id", "?")

                logger.info(
                    f"ğŸ“„ Doc {i+1}: {filename} (chunk {chunk_id}), "
                    f"distance={score:.4f}, similarity={similarity_score:.4f}"
                )

                if similarity_score >= self.score_threshold:
                    doc_score = DocumentScore(
                        filename=filename,
                        content=doc.page_content,
                        score=similarity_score,
                        relevance_reason=f"Similarity score: {similarity_score:.3f}",
                    )
                    document_scores.append(doc_score)
                    logger.info(
                        f"âœ… Document included: {filename} (score: {similarity_score:.3f})"
                    )
                else:
                    logger.info(
                        f"âŒ Document excluded: {filename} (score: {similarity_score:.3f} < {self.score_threshold})"
                    )

            logger.info(
                f"ğŸ‰ Retrieved {len(document_scores)} relevant documents above threshold"
            )
            return document_scores

        except Exception as e:
            logger.error(f"âŒ Error in document retrieval: {e}")
            return []

    def _prepare_context_aware_question(
        self, question: str, conversation_history: List[Dict[str, str]]
    ) -> str:
        """Prepare a context-aware question by combining current question with recent conversation history"""
        if not conversation_history:
            return question

        # Get last 3 exchanges for context (to avoid token limit issues)
        recent_history = (
            conversation_history[-6:]
            if len(conversation_history) > 6
            else conversation_history
        )

        context_parts = []
        for exchange in recent_history:
            if exchange.get("user"):
                context_parts.append(f"NgÆ°á»i dÃ¹ng Ä‘Ã£ há»i: {exchange['user']}")
            if exchange.get("assistant"):
                # Keep assistant responses short for context
                assistant_response = (
                    exchange["assistant"][:200] + "..."
                    if len(exchange["assistant"]) > 200
                    else exchange["assistant"]
                )
                context_parts.append(f"Trá»£ lÃ½ Ä‘Ã£ tráº£ lá»i: {assistant_response}")

        if context_parts:
            context_summary = "\n".join(context_parts)
            return f"Bá»‘i cáº£nh cuá»™c trÃ² chuyá»‡n:\n{context_summary}\n\nCÃ¢u há»i hiá»‡n táº¡i: {question}"

        return question

    def _format_conversation_history(
        self, conversation_history: List[Dict[str, str]]
    ) -> str:
        """Format conversation history for the prompt"""
        if not conversation_history:
            return ""

        # Get last 4 exchanges to keep context manageable
        recent_history = (
            conversation_history[-8:]
            if len(conversation_history) > 8
            else conversation_history
        )

        if not recent_history:
            return ""

        formatted_history = ["## ğŸ“ Lá»‹ch sá»­ cuá»™c trÃ² chuyá»‡n:"]

        for i, exchange in enumerate(recent_history):
            if exchange.get("user"):
                formatted_history.append(f"**NgÆ°á»i dÃ¹ng:** {exchange['user']}")
            if exchange.get("assistant"):
                # Truncate long responses
                assistant_response = (
                    exchange["assistant"][:300] + "..."
                    if len(exchange["assistant"]) > 300
                    else exchange["assistant"]
                )
                formatted_history.append(f"**Trá»£ lÃ½:** {assistant_response}")
            formatted_history.append("")  # Add spacing

        return "\n".join(formatted_history)

    def get_streaming_response(
        self, question: str, conversation_history: List[Dict[str, str]] = None
    ):
        """Get streaming response with enhanced RAG and conversation history"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ NEW REQUEST at {timestamp}")
        logger.info(f"â“ Question: {question}")
        logger.info(
            f"ğŸ’¬ Conversation history: {len(conversation_history) if conversation_history else 0} exchanges"
        )
        logger.info(f"{'='*60}")

        if conversation_history is None:
            conversation_history = []

        # Step 1: Check relevance (include conversation context for better relevance checking)
        logger.info("ğŸ“ STEP 1: Preparing context-aware question...")
        context_aware_question = self._prepare_context_aware_question(
            question, conversation_history
        )
        logger.info(f"ğŸ”„ Context-aware question: {context_aware_question[:200]}...")

        logger.info("ğŸ“ STEP 2: Checking relevance...")
        relevance_check = self._check_relevance(context_aware_question)

        if not relevance_check.relevant:
            logger.info("ğŸš« Question deemed not relevant - using basic LLM response")
            # If not relevant, use basic LLM response
            basic_prompt = PromptTemplate(
                template="""Báº¡n lÃ  trá»£ lÃ½ AI phÃ¡p lÃ½. CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng khÃ´ng liÃªn quan Ä‘áº¿n cÃ¡c tÃ i liá»‡u phÃ¡p lÃ½ vá» Viá»‡n Kiá»ƒm sÃ¡t cÃ³ sáºµn.

CÃ¢u há»i: {question}

HÃ£y tráº£ lá»i má»™t cÃ¡ch lá»‹ch sá»± vÃ  gá»£i Ã½ ngÆ°á»i dÃ¹ng Ä‘áº·t cÃ¢u há»i vá»:
- Äá»‹a chá»‰ trá»¥ sá»Ÿ cÃ¡c Viá»‡n Kiá»ƒm sÃ¡t
- ThÃ´ng tin liÃªn há»‡
- Tháº©m quyá»n cá»§a cÃ¡c Viá»‡n Kiá»ƒm sÃ¡t
- Quy Ä‘á»‹nh phÃ¡p lÃ½ liÃªn quan

Tráº£ lá»i:""",
                input_variables=["question"],
            )

            basic_chain = basic_prompt | self.streaming_llm | StrOutputParser()
            logger.info("ğŸ“¤ Returning basic LLM stream response")
            return basic_chain.stream({"question": question})

        # Step 2: Get relevant documents (use context-aware question for better retrieval)
        logger.info("ğŸ“ STEP 3: Retrieving relevant documents...")
        relevant_docs = self._get_relevant_documents(context_aware_question)

        if not relevant_docs:
            logger.warning("âš ï¸ No relevant documents found above threshold")
            # No relevant documents found
            no_docs_prompt = PromptTemplate(
                template="""CÃ¢u há»i cÃ³ liÃªn quan Ä‘áº¿n tÃ i liá»‡u phÃ¡p lÃ½ nhÆ°ng khÃ´ng tÃ¬m tháº¥y thÃ´ng tin cá»¥ thá»ƒ trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.

CÃ¢u há»i: {question}

HÃ£y tráº£ lá»i má»™t cÃ¡ch lá»‹ch sá»± vÃ  gá»£i Ã½ ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ cáº§n thÃ´ng tin cá»¥ thá»ƒ hÆ¡n.

Tráº£ lá»i:""",
                input_variables=["question"],
            )

            no_docs_chain = no_docs_prompt | self.streaming_llm | StrOutputParser()
            logger.info("ğŸ“¤ Returning no-docs LLM stream response")
            return no_docs_chain.stream({"question": question})

        # Step 3: Prepare context from relevant documents
        logger.info("ğŸ“ STEP 4: Preparing context from relevant documents...")
        context = "\n\n".join(
            [
                f"**TÃ i liá»‡u: {doc.filename}** (Äiá»ƒm sá»‘: {doc.score:.3f})\n{doc.content}"
                for doc in relevant_docs
            ]
        )
        logger.info(f"ğŸ“ Context prepared, total length: {len(context)} characters")
        logger.info(f"ğŸ“„ Using documents: {[doc.filename for doc in relevant_docs]}")

        # Step 4: Generate response with context and conversation history
        logger.info("ğŸ“ STEP 5: Formatting conversation history...")
        conversation_context = self._format_conversation_history(conversation_history)
        logger.info(
            f"ğŸ’¬ Conversation context length: {len(conversation_context)} characters"
        )

        logger.info("ğŸ“ STEP 6: Generating enhanced RAG response...")
        response_chain = self.enhanced_prompt | self.streaming_llm | StrOutputParser()
        logger.info("ğŸ“¤ Returning enhanced RAG stream response")
        logger.info(f"{'='*60}\n")

        return response_chain.stream(
            {
                "context": context,
                "question": question,
                "conversation_history": conversation_context,
            }
        )


# Initialize enhanced RAG agent lazily
_enhanced_agent = None


def get_enhanced_agent():
    """Lazy initialization of enhanced RAG agent"""
    global _enhanced_agent
    if _enhanced_agent is None:
        logger.info("ğŸ¬ Starting Enhanced RAG Agent initialization...")
        _enhanced_agent = EnhancedRAGAgent()
        logger.info("ğŸŒŸ Enhanced RAG Agent is ready to serve requests!")
    return _enhanced_agent


def get_streaming_response(question: str):
    """
    Gets a streaming response from the LLM.
    The original RAG functionality has been disabled to allow deployment.
    """
    return chain.stream({"question": question})


def get_enhanced_streaming_response(
    question: str, conversation_history: List[Dict[str, str]] = None
):
    """Public function to get enhanced streaming response with conversation history"""
    agent = get_enhanced_agent()
    return agent.get_streaming_response(question, conversation_history)
